{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "import nltk\n",
    "from os import path\n",
    "\n",
    "import context as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_name = path.join(c.DATA_PATH, 'капитал.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(book_name)\n",
    "raw = f.read()\n",
    "num_char = len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyMorphyTagger():\n",
    "    def __init__(self) -> None:\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "    def tag(self, word):\n",
    "        return self.morph.parse(word)[0].tag.POS\n",
    "\n",
    "# Тэггер, который никогда не даёт однозначного ответа\n",
    "class CustomTagger():\n",
    "    def tag(self, word):\n",
    "        # Источник - рандомные школьные презентации в интернете\n",
    "        adj_end = ['ой', 'ий', 'ый', 'ого', 'его', 'ому', 'ему', 'им', 'ым', 'ом', 'ем', 'ое', 'ее', 'ая', 'яя', 'ей', 'ую', 'юю', 'ие', 'ые', 'их', 'ых', 'ими', 'ыми']\n",
    "        adv_beg = ['на', 'за', 'в', 'из', 'ис', 'до', 'с']\n",
    "        adv_end = ['о', 'а']\n",
    "        vrb_end = ['у', 'ю', 'ем', 'ешь', 'ете', 'ет', 'ют', 'ут', 'у', 'ю', 'им', 'ишь', 'ите', 'ит', 'ят', 'ат']\n",
    "        vrb_inf_end = ['ть', 'ться', 'тся']\n",
    "\n",
    "        res = []\n",
    "        if CustomTagger.check_end(word, adv_end) and CustomTagger.check_beg(word, adv_beg):\n",
    "            res.append('ADV')\n",
    "\n",
    "        if CustomTagger.check_end(word, adj_end):\n",
    "            res.append('ADJ')\n",
    "       \n",
    "        if CustomTagger.check_end(word, vrb_end) or CustomTagger.check_end(word, vrb_inf_end):\n",
    "            res.append('VERB')\n",
    "        \n",
    "        # если слово не наречье и не глагол в инфинитиве, возможно, оно существительное\n",
    "        if ('ADV' not in res) and not CustomTagger.check_end(word, vrb_inf_end):\n",
    "            res.append('NOUN')\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def check_end(word, endings):\n",
    "        for end in endings:\n",
    "            if word.endswith(end):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def check_beg(word, endings):\n",
    "        for end in endings:\n",
    "            if word.startswith(end):\n",
    "                return True\n",
    "        return False\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.NLTKWordTokenizer()\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_text_stats(words, unique_words, name, stopwords = set()):\n",
    "    # пробуем очистить от стопслов без предварительной нормализации\n",
    "    words_count = len(words)\n",
    "    unique_words_clean = unique_words - stopwords\n",
    "\n",
    "    # пробуем очистить от стопслов после нормализации\n",
    "    unique_words_norm = {morph.parse(w)[0].normal_form for w in unique_words}\n",
    "    unique_words_norm_clean = unique_words_norm - stopwords\n",
    "\n",
    "    unique_words_count = len(unique_words_clean)\n",
    "    unique_words_norm_count = len(unique_words_norm_clean)\n",
    "\n",
    "    print(f'Статистика по файлу {name}')\n",
    "    print('    Количество слов в тексте:', words_count)\n",
    "    print('    Количество уникальных слов:', unique_words_count)\n",
    "    print('    Количество уникальных слов в нормальной форме:', unique_words_norm_count)\n",
    "    print('    Лексическое разнообразие:', unique_words_count / words_count)\n",
    "    print('    Лексическое разнообразие в нормальной форме:', unique_words_norm_count / words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статистика по файлу data\\капитал.txt\n",
      "    Количество слов в тексте: 360993\n",
      "    Количество уникальных слов: 27972\n",
      "    Количество уникальных слов в нормальной форме: 11911\n",
      "    Лексическое разнообразие: 0.07748626704672944\n",
      "    Лексическое разнообразие в нормальной форме: 0.03299509962797063\n",
      "Статистика по файлу чистый_data\\капитал.txt\n",
      "    Количество слов в тексте: 360993\n",
      "    Количество уникальных слов: 27823\n",
      "    Количество уникальных слов в нормальной форме: 11806\n",
      "    Лексическое разнообразие: 0.07707351666098788\n",
      "    Лексическое разнообразие в нормальной форме: 0.032704235262179596\n"
     ]
    }
   ],
   "source": [
    "words = tokenizer.tokenize(raw)\n",
    "words = [w.lower() for w in words]\n",
    "words = [w for w in words if w.isalpha()]\n",
    "\n",
    "unique_words = set(words)\n",
    "stopwords = set(nltk.corpus.stopwords.words('russian'))\n",
    "\n",
    "print_text_stats(words, unique_words, book_name)\n",
    "print_text_stats(words, unique_words, 'чистый_' + book_name, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выбраны слова для проверки:  ['уравновешенным', 'долями', 'трудностей', 'создание', 'привел', 'достаточный', 'стоила', 'машина']\n",
      "Проверяем PyMorphyTagger:\n",
      "    уравновешенным PRTF\n",
      "    долями NOUN\n",
      "    трудностей NOUN\n",
      "    создание NOUN\n",
      "    привел VERB\n",
      "    достаточный ADJF\n",
      "    стоила VERB\n",
      "    машина NOUN\n",
      "Проверяем CustomTagger:\n",
      "    уравновешенным ['ADJ', 'NOUN']\n",
      "    долями ['NOUN']\n",
      "    трудностей ['ADJ', 'NOUN']\n",
      "    создание ['ADJ', 'NOUN']\n",
      "    привел ['NOUN']\n",
      "    достаточный ['ADJ', 'NOUN']\n",
      "    стоила ['ADV']\n",
      "    машина ['NOUN']\n"
     ]
    }
   ],
   "source": [
    "taggers = [PyMorphyTagger(), CustomTagger()]\n",
    "selected_words = list(unique_words)[1010:1018]\n",
    "print('Выбраны слова для проверки: ', selected_words)\n",
    "\n",
    "for tagger in taggers:\n",
    "    print(f'Проверяем {tagger.__class__.__name__}:')\n",
    "    for w in selected_words:\n",
    "        print('   ', w, tagger.tag(w))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd0fe5abff45bced992256837a1973600adefa1552ea1edefa1a1225dcfc10d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
