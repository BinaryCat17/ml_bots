{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('punkt')\n",
    "#nltk.download('gutenberg')\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96825"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "milton = nltk.corpus.gutenberg.words('milton-paradise.txt')\n",
    "len(milton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m num_word \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(nltk\u001b[39m.\u001b[39mcorpus\u001b[39m.\u001b[39mgutenberg\u001b[39m.\u001b[39mwords(file))\n\u001b[0;32m      4\u001b[0m num_sent \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(nltk\u001b[39m.\u001b[39mcorpus\u001b[39m.\u001b[39mgutenberg\u001b[39m.\u001b[39msents(file))\n\u001b[1;32m----> 5\u001b[0m num_vocab \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39;49m(w\u001b[39m.\u001b[39;49mlower() \u001b[39mfor\u001b[39;49;00m w \u001b[39min\u001b[39;49;00m nltk\u001b[39m.\u001b[39;49mcorpus\u001b[39m.\u001b[39;49mgutenberg\u001b[39m.\u001b[39;49msents(file)))\n",
      "Cell \u001b[1;32mIn [8], line 5\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m num_word \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(nltk\u001b[39m.\u001b[39mcorpus\u001b[39m.\u001b[39mgutenberg\u001b[39m.\u001b[39mwords(file))\n\u001b[0;32m      4\u001b[0m num_sent \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(nltk\u001b[39m.\u001b[39mcorpus\u001b[39m.\u001b[39mgutenberg\u001b[39m.\u001b[39msents(file))\n\u001b[1;32m----> 5\u001b[0m num_vocab \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(w\u001b[39m.\u001b[39;49mlower() \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m nltk\u001b[39m.\u001b[39mcorpus\u001b[39m.\u001b[39mgutenberg\u001b[39m.\u001b[39msents(file)))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "for file in nltk.corpus.gutenberg.fileids():\n",
    "    num_char = len(nltk.corpus.gutenberg.raw(file))\n",
    "    num_word = len(nltk.corpus.gutenberg.words(file))\n",
    "    num_sent = len(nltk.corpus.gutenberg.sents(file))\n",
    "    num_vocab = len(set(w.lower() for w in nltk.corpus.gutenberg.sents(file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ай да А.С. Пушкин!', 'Ай да хитрец!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Ай да А.С. Пушкин! Ай да хитрец!'\n",
    "nltk.sent_tokenize(text, language='russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ты плохой муж. Я бы этим особо не хвастался.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Ты плохой муж. Я бы этим особо не хвастался.'\n",
    "nltk.sent_tokenize(text, language='russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['и',\n",
       " 'в',\n",
       " 'во',\n",
       " 'не',\n",
       " 'что',\n",
       " 'он',\n",
       " 'на',\n",
       " 'я',\n",
       " 'с',\n",
       " 'со',\n",
       " 'как',\n",
       " 'а',\n",
       " 'то',\n",
       " 'все',\n",
       " 'она',\n",
       " 'так',\n",
       " 'его',\n",
       " 'но',\n",
       " 'да',\n",
       " 'ты',\n",
       " 'к',\n",
       " 'у',\n",
       " 'же',\n",
       " 'вы',\n",
       " 'за',\n",
       " 'бы',\n",
       " 'по',\n",
       " 'только',\n",
       " 'ее',\n",
       " 'мне',\n",
       " 'было',\n",
       " 'вот',\n",
       " 'от',\n",
       " 'меня',\n",
       " 'еще',\n",
       " 'нет',\n",
       " 'о',\n",
       " 'из',\n",
       " 'ему',\n",
       " 'теперь',\n",
       " 'когда',\n",
       " 'даже',\n",
       " 'ну',\n",
       " 'вдруг',\n",
       " 'ли',\n",
       " 'если',\n",
       " 'уже',\n",
       " 'или',\n",
       " 'ни',\n",
       " 'быть',\n",
       " 'был',\n",
       " 'него',\n",
       " 'до',\n",
       " 'вас',\n",
       " 'нибудь',\n",
       " 'опять',\n",
       " 'уж',\n",
       " 'вам',\n",
       " 'ведь',\n",
       " 'там',\n",
       " 'потом',\n",
       " 'себя',\n",
       " 'ничего',\n",
       " 'ей',\n",
       " 'может',\n",
       " 'они',\n",
       " 'тут',\n",
       " 'где',\n",
       " 'есть',\n",
       " 'надо',\n",
       " 'ней',\n",
       " 'для',\n",
       " 'мы',\n",
       " 'тебя',\n",
       " 'их',\n",
       " 'чем',\n",
       " 'была',\n",
       " 'сам',\n",
       " 'чтоб',\n",
       " 'без',\n",
       " 'будто',\n",
       " 'чего',\n",
       " 'раз',\n",
       " 'тоже',\n",
       " 'себе',\n",
       " 'под',\n",
       " 'будет',\n",
       " 'ж',\n",
       " 'тогда',\n",
       " 'кто',\n",
       " 'этот',\n",
       " 'того',\n",
       " 'потому',\n",
       " 'этого',\n",
       " 'какой',\n",
       " 'совсем',\n",
       " 'ним',\n",
       " 'здесь',\n",
       " 'этом',\n",
       " 'один',\n",
       " 'почти',\n",
       " 'мой',\n",
       " 'тем',\n",
       " 'чтобы',\n",
       " 'нее',\n",
       " 'сейчас',\n",
       " 'были',\n",
       " 'куда',\n",
       " 'зачем',\n",
       " 'всех',\n",
       " 'никогда',\n",
       " 'можно',\n",
       " 'при',\n",
       " 'наконец',\n",
       " 'два',\n",
       " 'об',\n",
       " 'другой',\n",
       " 'хоть',\n",
       " 'после',\n",
       " 'над',\n",
       " 'больше',\n",
       " 'тот',\n",
       " 'через',\n",
       " 'эти',\n",
       " 'нас',\n",
       " 'про',\n",
       " 'всего',\n",
       " 'них',\n",
       " 'какая',\n",
       " 'много',\n",
       " 'разве',\n",
       " 'три',\n",
       " 'эту',\n",
       " 'моя',\n",
       " 'впрочем',\n",
       " 'хорошо',\n",
       " 'свою',\n",
       " 'этой',\n",
       " 'перед',\n",
       " 'иногда',\n",
       " 'лучше',\n",
       " 'чуть',\n",
       " 'том',\n",
       " 'нельзя',\n",
       " 'такой',\n",
       " 'им',\n",
       " 'более',\n",
       " 'всегда',\n",
       " 'конечно',\n",
       " 'всю',\n",
       " 'между']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "russian_stopwords = nltk.corpus.stopwords.words('russian')\n",
    "russian_stopwords\n",
    "\n",
    "#punctuation = nltk.punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mword_tokenize(text\u001b[39m.\u001b[39mlower())\n\u001b[1;32m----> 2\u001b[0m tokens \u001b[39m=\u001b[39m [token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m russian_stopwords \u001b[39mand\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m string\u001b[39m.\u001b[39mpunctuation]\n",
      "Cell \u001b[1;32mIn [17], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mword_tokenize(text\u001b[39m.\u001b[39mlower())\n\u001b[1;32m----> 2\u001b[0m tokens \u001b[39m=\u001b[39m [token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m russian_stopwords \u001b[39mand\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m string\u001b[39m.\u001b[39mpunctuation]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'string' is not defined"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text.lower())\n",
    "nltk\n",
    "tokens = [token for token in tokens if token not in russian_stopwords and token not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good', 'muffins', 'cost', '$', '3,88', 'in', 'New', 'York', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '''Good muffins cost $3,88\\nin New York.'''\n",
    "nltk.tokenize.TreebankWordTokenizer().tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'a',\n",
       " 'little',\n",
       " 'or',\n",
       " 'a',\n",
       " 'little',\n",
       " 'bit',\n",
       " 'or',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'in',\n",
       " 'sprite',\n",
       " 'of']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.MWETokenizer([('a', 'little'), ('a', 'little', 'bit'), ('a', 'lot')])\n",
    "tokenizer.add_mwe(('in', 'spite', 'of'))\n",
    "tokenizer.tokenize('In a little or a little bit or a lot in sprite of')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100-115 предложение \n",
    "# прообуем все токенизаторы, описывая особенности токенизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236736"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = [\n",
    "    # поддержка коротких сообщений в Twitter (смайлики, как текстовые, так и обычные, теги пользователей и т.п.)\n",
    "    nltk.tokenize.TweetTokenizer(),\n",
    "    # Улучшенная версия TreebankWordTokenizer, нужно, чтобы текст уже был разбит на предложения. Обрабатывает многие сокращения.\n",
    "    # Применение регулярных выражений изменит исходный текст без возможности восстановления\n",
    "    nltk.tokenize.NLTKWordTokenizer(),\n",
    "    # Разбивает слова по слогам, основываясь на принципе правильности и максимизации наборов.\n",
    "    # Конструктор принимает список доступных токенов и список всех гласных в языке. Принимает слово для токенизации.\n",
    "    nltk.tokenize.LegalitySyllableTokenizer(nltk.corpus.words.words()),\n",
    "    # Берет строку, которая уже была разделена на токены, и повторно использует ее,\n",
    "    # объединяя выражения из нескольких слов в отдельные токены, используя лексику MWEs.\n",
    "    # Принимает список выражений\n",
    "    nltk.tokenize.MWETokenizer([('a', 'little'), ('a', 'little', 'bit')]),\n",
    "    # Этот токенизатор делит текст на список предложений, используя неконтролируемый алгоритм для построения модели для слов-сокращений,\n",
    "    # словосочетаний и слов, с которых начинаются предложения.\n",
    "    # Он должен быть обучен на большом наборе открытого текста на целевом языке, прежде чем его можно будет использовать.\n",
    "    nltk.data.load('tokenizers/punkt/english.pickle'),\n",
    "    # RegexpTokenizer разбивает строку на подстроки, используя регулярное выражение.\n",
    "    # Например, следующий токенизатор формирует токены из буквенных последовательностей, денежных выражений и любых других последовательностей, не содержащих пробелов:\n",
    "    nltk.RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+'),\n",
    "    # SExprTokenizer используется для поиска выражений в скобках в строке.\n",
    "    # В частности, он делит строку на последовательность подстрок, которые являются либо выражениями в скобках (включая любые вложенные выражения в скобках)\n",
    "    nltk.SExprTokenizer(),\n",
    "    # Токенезирует строку, используя пробел в качестве разделителя, который совпадает с s.split(' ').\n",
    "    nltk.tokenize.SpaceTokenizer(),\n",
    "    # Sonority Sequencing Principle (SSP). Принимает слово и разбивает его на слоги фонетически.\n",
    "    nltk.SyllableTokenizer(),\n",
    "    # Стэнфордский токенизатор\n",
    "    nltk.StanfordTokenizer(),\n",
    "    # TextTiling – это метод автоматического разделения полноразмерных текстовых документов на последовательные блоки,\n",
    "    # состоящие из нескольких абзацев, которые представляют собой отрывки или подтемы.\n",
    "    # Алгоритм предполагает, что в ходе описания подтемы используется определенный набор слов, и когда подтема изменяется,\n",
    "    # значительная часть словарного запаса также изменяется.\n",
    "    # w (int) – размер псевдопредложения, на которые разбивается исходный текст; k (int) – размер блока (в предложениях),\n",
    "    # используемого в методе сравнения блоков. Исходный текст можно очистить от стоп-слов, используя параметр stopwords\n",
    "    nltk.TextTilingTokenizer()\n",
    "    # На вход которого подается по одному предложению в строке. Работа токенизатора основана на последовательном\n",
    "    # применении к исходному тексту списка регулярных выражений. Не всегда отделяет знаки препинания от слов.\n",
    "\n",
    "    # Токенизатор Treebank использует регулярные выражения для токенизации текста, как в Penn Treebank.\n",
    "    # Эта реализация является портом сценария tokenizer sed, написанного Робертом Макинтайром и доступного по адресу\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd0fe5abff45bced992256837a1973600adefa1552ea1edefa1a1225dcfc10d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
